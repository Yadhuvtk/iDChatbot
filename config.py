"""
Central configuration for SOP Chatbot (V5 â€” Accuracy-First).
All tunable parameters in one place.
"""
import os
import torch

# â”€â”€ Paths â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_DIR = os.path.join(BASE_DIR, "data")

# Data Files
SOP_DATA_FILE = os.path.join(DATA_DIR, "sop_data.json")
RAW_DATA_FILE = os.path.join(DATA_DIR, "data.txt")
CONVERTED_JSON_PATH = os.path.join(DATA_DIR, "converted.json") # Added for pipeline compatibility

# Index Files
FAISS_INDEX_FILE = os.path.join(DATA_DIR, "sop_index.faiss")
ID_MAP_FILE = os.path.join(DATA_DIR, "id_map.json")
BM25_INDEX_FILE = os.path.join(DATA_DIR, "bm25_index.pkl")
FUSION_MODEL_FILE = os.path.join(DATA_DIR, "fusion_model.pkl")
CALIBRATOR_FILE = os.path.join(DATA_DIR, "calibrator.pkl")

# â”€â”€ Hardware Settings â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
print(f"ğŸ–¥ï¸  Using device: {DEVICE}")

# â”€â”€ Embedding Model (V5: Single Strong Retriever) â”€â”€â”€â”€â”€
# Check for local manual download first
LOCAL_MODEL_PATH = os.path.join(BASE_DIR, "models", "bge-m3")
if os.path.exists(LOCAL_MODEL_PATH):
    EMBEDDING_MODEL = LOCAL_MODEL_PATH
    print(f"ğŸ“‚ Using local embedding model: {EMBEDDING_MODEL}")
else:
    # BGE-M3: dense + sparse in one model (downloads from HF)
    EMBEDDING_MODEL = "BAAI/bge-m3"

RERANKER_NAME = "cross-encoder/ms-marco-MiniLM-L-6-v2"
EMBEDDING_DIM = 1024  # BGE-M3 dense dimension

# Prefix for BGE queries (used by fallback mode)
QUERY_PREFIX = "Represent this sentence for searching relevant passages: "

# â”€â”€ Retrieval (V5) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# The threshold below which we reject answers (Zero Hallucination)
SIMILARITY_THRESHOLD = 0.45   

TOP_K_RETRIEVAL = 20          # Candidates for reranking
TOP_K_FINAL = 1               # Final output

# Fusion default weights: [dense, sparse, bm25]
FUSION_WEIGHTS = [0.5, 0.2, 0.3]

# â”€â”€ LLM Rewrite (Ben-Chat Style) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
USE_LLM_REWRITE = True
OLLAMA_BASE_URL = "http://localhost:11434"
# Using Qwen 2.5 (1.5B) - fast and accurate for rewriting
OLLAMA_MODEL = "mistral" 

# â”€â”€ Messages â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
REJECTION_MESSAGE = (
    "This question is not covered in the SOP.\n"
    "Please contact HR."
)

# â”€â”€ Web UI â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
APP_HOST = "0.0.0.0"
APP_PORT = 7860

# â”€â”€ COMPATIBILITY ALIASES â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# These ensure compatibility with core/pipeline.py and core/rewriter.py
FALLBACK_MESSAGE = REJECTION_MESSAGE
RETRIEVAL_K = TOP_K_RETRIEVAL
MODEL_NAME = EMBEDDING_MODEL 
INDEX_PATH = FAISS_INDEX_FILE
METADATA_PATH = ID_MAP_FILE